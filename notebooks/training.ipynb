{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "from pathlib import PurePath\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from asteroid.data import TimitDataset, TimitCleanDataset, RandomMixtureDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import optim\n",
    "from pytorch_lightning import Trainer, seed_everything, loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from asteroid_filterbanks.transforms import mag\n",
    "from asteroid.engine import System\n",
    "from asteroid.losses import singlesrc_neg_sisdr\n",
    "\n",
    "from asteroid import DCUNet, DCCRNet\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE       = 64     # could be more on cluster, test if larger one work\n",
    "SAMPLE_RATE      = 8000   # as agreed upon\n",
    "#CROP_LEN         = 8192   # slightly more than a second, guaranteed to be less than the shortest clip in TIMIT\n",
    "CROP_LEN         = 24000  # average track len in TIMIT\n",
    "SEED             = 42     # magic number :)    \n",
    "\n",
    "# directory to cache fixed mixtures\n",
    "TIMIT_CACHE_DIR = '/import/vision-eddydata/dm005_tmp/mixed_wavs_asteroid2'\n",
    "# directory with train noises (n116-n120)\n",
    "DRONE_NOISE_DIR = '../../../datasets/noises-train-drones'\n",
    "# fixed SNRs for validation set\n",
    "TRAIN_SNRS = [-25, -20, -15, -10, -5]\n",
    "\n",
    "def sisdr_loss_wrapper(est_target, target):\n",
    "    return singlesrc_neg_sisdr(est_target.squeeze(1), target).mean()\n",
    "\n",
    "def train_val_split(ds, val_fraction=0.1, random_seed=SEED):\n",
    "    assert val_fraction > 0 and val_fraction < 0.5\n",
    "    len_train = int(len(ds) * (1 - val_fraction))\n",
    "    len_val = len(ds) - len_train\n",
    "    return random_split(ds, [len_train, len_val], generator=torch.Generator().manual_seed(random_seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample TIMIT dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMIT_DIR = PurePath('../../../datasets/TIMIT')\n",
    "TIMIT_DIR_8kHZ = PurePath('/import/vision-eddydata/dm005_tmp/TIMIT_8kHZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resampling training data: 100%|██████████| 4620/4620 [00:23<00:00, 195.48it/s]\n",
      "Resampling test data: 100%|██████████| 1680/1680 [00:08<00:00, 193.00it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(TIMIT_DIR_8kHZ, exist_ok=True)\n",
    "shutil.copyfile(TIMIT_DIR / 'train_data.csv', TIMIT_DIR_8kHZ / 'train_data.csv')\n",
    "shutil.copyfile(TIMIT_DIR / 'test_data.csv', TIMIT_DIR_8kHZ / 'test_data.csv')\n",
    "\n",
    "data_dir_in = TIMIT_DIR / 'data'\n",
    "data_dir_out = TIMIT_DIR_8kHZ / 'data'\n",
    "\n",
    "def resample(ds, dir_in, dir_out, message='Resampling'):\n",
    "    dl = DataLoader(ds, num_workers=10)\n",
    "    for wav, path in tqdm(dl, message):\n",
    "        path = PurePath(path[0])\n",
    "        out_path = dir_out / path.relative_to(dir_in)\n",
    "        os.makedirs(out_path.parent, exist_ok=True)\n",
    "        sf.write(file=out_path, data=wav[0].numpy(), samplerate=SAMPLE_RATE)\n",
    "\n",
    "timit_train = TimitCleanDataset(TIMIT_DIR, subset='train', sample_rate=SAMPLE_RATE)\n",
    "resample(timit_train, data_dir_in, data_dir_out, 'Resampling training data')\n",
    "\n",
    "timit_test = TimitCleanDataset(TIMIT_DIR, subset='test', sample_rate=SAMPLE_RATE)\n",
    "resample(timit_test, data_dir_in, data_dir_out, 'Resampling test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I wanted to do this stuff with random training data, but unfortunately the start of the epoch seems to be very \n",
    "# slow with this approach. I don't yet understand why this happens. Let's stick to the fixed dataset for now\n",
    "\n",
    "# Reproducibility - fix all random seeds\n",
    "# seed_everything(SEED)\n",
    "\n",
    "# # Load clean data and split it into train and val\n",
    "# timit = TimitCleanDataset(TIMIT_DIR_8kHZ, subset='train', sample_rate=SAMPLE_RATE)\n",
    "# timit_train, timit_val = train_val_split(timit, val_fraction=0.1, random_seed=SEED)\n",
    "\n",
    "# # Training data mixes crops randomly on the fly with random SNR in range (effectively infinite training data)\n",
    "# timit_train = RandomMixtureDataset(timit_train, DRONE_NOISE_DIR, random_seed=SEED, snr_range=(-25, -5),\n",
    "#                                    crop_length=CROP_LEN, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "# # Validation data is fixed (for stability): mix every clean clip with all the noises in the folder\n",
    "# # You can add the argument `prefetch_mixtures=False` to cancel iterating over the whole dataset to save\n",
    "# # the mixtures in the cache folder in advance\n",
    "# # Argument `mixtures_per_clean` regulates with how many different noise files each clean file will be mixed\n",
    "# timit_val = TimitDataset.load_with_cache(\n",
    "#      timit_val, DRONE_NOISE_DIR, cache_dir=TIMIT_CACHE_DIR, snrs=TRAIN_SNRS, root_seed=SEED,\n",
    "#      mixtures_per_clean=3, dset_name='valid-drones', sample_rate=SAMPLE_RATE,\n",
    "#      subset='train', crop_length=CROP_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing datasets: 100%|██████████| 5/5 [06:22<00:00, 76.50s/it]\n",
      "Load samples: 100%|██████████| 115500/115500 [05:53<00:00, 326.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track lengths stats: total 2772000000, mean 24000.0, median 24000.0, min 24000, max 24000\n",
      "Tracks in total: 115500\n",
      "Total audio duration: 48:7:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility - fix all random seeds\n",
    "seed_everything(SEED)\n",
    "\n",
    "timit_train_drones = TimitDataset.load_with_cache(\n",
    "    TIMIT_DIR_8kHZ, DRONE_NOISE_DIR,\n",
    "    cache_dir=TIMIT_CACHE_DIR, snrs=TRAIN_SNRS, root_seed=SEED,\n",
    "    mixtures_per_clean=5, dset_name='train-drones', sample_rate=SAMPLE_RATE,\n",
    "    subset='train', crop_length=CROP_LEN)\n",
    "\n",
    "timit_train, timit_val = train_val_split(timit_train_drones, val_fraction=0.1, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 5\n",
    "train_loader = DataLoader(timit_train, shuffle=True, batch_size=BATCH_SIZE,\n",
    "                          num_workers=NUM_WORKERS, drop_last=True)\n",
    "val_loader = DataLoader(timit_val, batch_size=BATCH_SIZE,\n",
    "                        num_workers=NUM_WORKERS, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model, optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some random parameters, does it look sensible?\n",
    "LR = 1e-3\n",
    "REDUCE_LR_PATIENCE = 3\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "MAX_EPOCHS = 500\n",
    "\n",
    "# the model here should be constructed in the script accordingly to the passed config (including the model type)\n",
    "# most of the models accept `sample_rate` parameter for encoders, which is important (default is 16000, override)\n",
    "model = DCUNet(\"DCUNet-20\", fix_length_mode=\"trim\", sample_rate=SAMPLE_RATE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=REDUCE_LR_PATIENCE)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=EARLY_STOP_PATIENCE)\n",
    "\n",
    "# Probably we also need to subclass `System`, in order to log the target metrics on the validation set (PESQ/STOI)\n",
    "system = System(model, optimizer, sisdr_loss_wrapper, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    }
   ],
   "source": [
    "# log dir and model name are also part of the config, of course\n",
    "LOG_DIR = 'logs'\n",
    "logger = pl_loggers.TensorBoardLogger(LOG_DIR, name='TIMIT-drones-DCUNET-20-proper', version=1)\n",
    "\n",
    "# choose the proper accelerator for JADE, probably `ddp` (also, `auto_select_gpus=True` might be useful)\n",
    "trainer = Trainer(max_epochs=MAX_EPOCHS, gpus=-1, accelerator='dp',\n",
    "                  logger=logger, callbacks=[early_stopping], deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | DCUNet | 3.5 M \n",
      "---------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224ac12cd31c4f45a2b928c38b60ff18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the models fully with all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.serialize(), 'dcunet_20_proper_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
