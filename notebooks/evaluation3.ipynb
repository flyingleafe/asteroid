{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa as lr\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "from librosa import display as lrd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import deque\n",
    "from functools import partial\n",
    "from torch.multiprocessing import Process, Queue, cpu_count\n",
    "from queue import Empty\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split, Subset\n",
    "from asteroid.data import TimitDataset\n",
    "from asteroid.data.utils import CachedWavSet, FixedMixtureSet\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch import optim\n",
    "from pytorch_lightning import Trainer, loggers as pl_loggers\n",
    "from asteroid_filterbanks.transforms import mag\n",
    "\n",
    "from asteroid import DCUNet, DCCRNet, DPRNNTasNet, ConvTasNet\n",
    "from asteroid.utils.notebook_utils import show_wav\n",
    "\n",
    "from asteroid.metrics import get_metrics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMIT_DIR_8kHZ = '/import/vision-eddydata/dm005_tmp/TIMIT_8kHZ'\n",
    "TEST_NOISE_DIR = '../../../datasets/noises-test-drones'\n",
    "SAMPLE_RATE    = 8000\n",
    "TEST_SNRS      = [-30, -25, -20, -15, -10, -5, 0]\n",
    "SEED           = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precaching audio: 100%|██████████| 2/2 [00:00<00:00, 136.48it/s]\n"
     ]
    }
   ],
   "source": [
    "timit_test_clean = TimitDataset(TIMIT_DIR_8kHZ, subset='test', sample_rate=SAMPLE_RATE, with_path=False)\n",
    "noises_test = CachedWavSet(TEST_NOISE_DIR, sample_rate=SAMPLE_RATE, precache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_test = FixedMixtureSet(timit_test_clean, noises_test, snrs=TEST_SNRS, random_seed=SEED, with_snr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(batch, metrics, including='output', sample_rate=8000):\n",
    "    mix, clean, estimate, snr = batch\n",
    "    metrics = get_metrics(mix.numpy(), clean.numpy(), estimate.numpy(),\n",
    "                          sample_rate=sample_rate, metrics_list=metrics, including=including)\n",
    "    metrics['snr'] = snr[0].item()\n",
    "    return metrics\n",
    "\n",
    "def data_feed_process(queue, signal_queue, model, test_set):\n",
    "    loader = DataLoader(test_set, num_workers=2)\n",
    "    \n",
    "    if model is not None:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    def model_eval(mix):\n",
    "        if model is None:\n",
    "            return mix\n",
    "        else:\n",
    "            return model(mix.cuda()).squeeze(1).detach().cpu()\n",
    "    \n",
    "#     print('FEEDING STARTED')\n",
    "    \n",
    "    i = 0\n",
    "    for mix, clean, snr in loader:\n",
    "#         print('lets eval! ' + str(i))\n",
    "        enh = model_eval(mix)\n",
    "#         print('enh ready! ' + str(i))\n",
    "        queue.put((mix, clean, enh, snr))\n",
    "#         print('put into queue! ' + str(i))\n",
    "        i += 1\n",
    "        \n",
    "#     print('FEEDING DONE')\n",
    "    # wait for a signal to end the process\n",
    "    signal_queue.get()\n",
    "        \n",
    "def eval_process(proc_idx, input_queue, output_queue, **kwargs):\n",
    "#     print(f'WORKER {proc_idx} STARTED')\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch = input_queue.get()\n",
    "            if batch is None:\n",
    "#                 print(f'WORKER {proc_idx} IS DONE')\n",
    "                break\n",
    "            else:\n",
    "                res = _eval(batch, **kwargs)\n",
    "#                 print(f'WORKER {proc_idx}: EVALED {i}')\n",
    "                output_queue.put(res)\n",
    "#                 print(f'WORKER {proc_idx} RESULT SENT {i}')\n",
    "                i += 1\n",
    "        except Empty:\n",
    "            print(f'WORKER {proc_idx} empty! {i}')\n",
    "            time.sleep(0.1)\n",
    "\n",
    "def evaluate_model(model, test_set, num_workers=None, metrics=['pesq', 'stoi', 'si_sdr'],\n",
    "                   sample_rate=8000, max_queue_size=100):\n",
    "    \n",
    "    if num_workers is None:\n",
    "        num_workers = cpu_count()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['snr']+metrics)\n",
    "    ds_len = len(test_set)\n",
    "    including = 'input' if model is None else 'output'\n",
    "    \n",
    "    signal_queue = Queue()\n",
    "    input_queue = Queue(maxsize=max_queue_size)\n",
    "    output_queue = Queue(maxsize=max_queue_size)\n",
    "    \n",
    "    feed_pr = Process(target=data_feed_process, args=(input_queue, signal_queue, model, test_set))\n",
    "    feed_pr.start()\n",
    "        \n",
    "    eval_prs = []\n",
    "    for i in range(num_workers-1):\n",
    "        pr = Process(target=eval_process, args=(i, input_queue, output_queue), kwargs={\n",
    "            'metrics': metrics,\n",
    "            'including': including,\n",
    "            'sample_rate': sample_rate\n",
    "        })\n",
    "        pr.start()\n",
    "        eval_prs.append(pr)\n",
    "    \n",
    "    for i in tqdm(range(ds_len), 'Evaluating and calculating scores'):\n",
    "        res = output_queue.get()\n",
    "        df = df.append(res, ignore_index=True)\n",
    "        \n",
    "    signal_queue.put(None)\n",
    "    for pr in eval_prs:\n",
    "        input_queue.put(None)\n",
    "    \n",
    "    feed_pr.join()\n",
    "    for pr in eval_prs:\n",
    "        pr.join()\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate_input(*args, **kwargs):\n",
    "    return evaluate_model(None, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names = {\n",
    "    'pesq': 'PESQ',\n",
    "    'stoi': 'STOI',\n",
    "    'si_sdr': 'SI-SDR',\n",
    "}\n",
    "\n",
    "model_labels = {\n",
    "    'input': 'Original noisy input',\n",
    "    'dcunet_20': 'DCUNet-20',\n",
    "    'dccrn': 'DCCRN',\n",
    "    'dprnn': 'DPRNN',\n",
    "    'conv_tasnet': 'Conv-TasNet',\n",
    "}\n",
    "\n",
    "def plot_results(dfs, figsize=(15, 5), metrics=['pesq', 'stoi', 'si_sdr'],\n",
    "                 plot_name=None): \n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(metrics), figsize=figsize)\n",
    "    \n",
    "    for model_name, df in dfs.items():\n",
    "        scores = df.groupby('snr').agg({\n",
    "            metric: ['mean', 'std', 'count'] for metric in metrics\n",
    "        })\n",
    "        \n",
    "        line_kwargs = {'marker': 'o', 'alpha': 0.8}\n",
    "        fill_kwargs = {}\n",
    "        if model_name == 'input':\n",
    "            line_kwargs = {'c': 'black', 'ls': '--'}\n",
    "            fill_kwargs = {'color': 'black'}\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            plt.sca(axes[i])\n",
    "            means = scores[metric]['mean']\n",
    "            stds = scores[metric]['std'].values / np.sqrt(scores[metric]['count'].values) * 3\n",
    "            xs = means.index\n",
    "            plt.plot(xs, means, label=model_labels[model_name], **line_kwargs)\n",
    "            plt.fill_between(xs, means - stds, means + stds, alpha=0.2, **fill_kwargs)\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.sca(axes[i])\n",
    "        plt.grid(which='both')\n",
    "        plt.title(metrics_names[metric])\n",
    "        plt.xlabel('SNR, dB')\n",
    "        if i == 0:\n",
    "            plt.legend()\n",
    "    \n",
    "    if plot_name is not None:\n",
    "        plt.savefig(plot_name, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'input': None,\n",
    "    'dcunet_20': DCUNet.from_pretrained('../../../workspace/models/dcunet_20_random_v2.pt'),\n",
    "    'dccrn': DCCRNet.from_pretrained('../../../workspace/models/dccrn_random_v1.pt'),\n",
    "    'dprnn': DPRNNTasNet.from_pretrained('../../../workspace/models/dprnn_model.pt'),\n",
    "    'conv_tasnet': ConvTasNet.from_pretrained('../../../workspace/models/convtasnet_model.pt'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Original noisy input\n",
      "Results already available\n",
      "Evaluating DCUNet-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating and calculating scores: 100%|██████████| 23520/23520 [24:43<00:00, 15.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DCCRN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating and calculating scores: 100%|██████████| 23520/23520 [32:27<00:00, 12.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DPRNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating and calculating scores: 100%|██████████| 23520/23520 [33:20<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Conv-TasNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating and calculating scores:  96%|█████████▋| 22685/23520 [24:02<00:45, 18.20it/s]"
     ]
    }
   ],
   "source": [
    "results_dfs = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'Evaluating {model_labels[model_name]}')\n",
    "    csv_path = f'../../../workspace/eval_results/{model_name}.csv'\n",
    "    \n",
    "    if os.path.isfile(csv_path):\n",
    "        print('Results already available')\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        df = evaluate_model(model, timit_test)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "    results_dfs[model_name] = df\n",
    "    \n",
    "plot_results(results_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
