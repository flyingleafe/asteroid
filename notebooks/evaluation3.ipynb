{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa as lr\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n",
    "from librosa import display as lrd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from collections import deque\n",
    "from functools import partial\n",
    "from multiprocessing import Process, Queue, cpu_count\n",
    "from queue import Empty\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, random_split\n",
    "from asteroid.data import TimitDataset\n",
    "from asteroid.data.utils import CachedWavSet, FixedMixtureSet\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch import optim\n",
    "from pytorch_lightning import Trainer, loggers as pl_loggers\n",
    "from asteroid_filterbanks.transforms import mag\n",
    "\n",
    "from asteroid import DCUNet, DCCRNet\n",
    "from asteroid.utils.notebook_utils import show_wav\n",
    "\n",
    "from asteroid.metrics import get_metrics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMIT_DIR_8kHZ = '../../datasets/TIMIT_8kHz'\n",
    "TEST_NOISE_DIR = '../../datasets/noises-test-drones'\n",
    "SAMPLE_RATE    = 8000\n",
    "TEST_SNRS      = [-30, -25, -20, -15, -10, -5, 0]\n",
    "SEED           = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precaching audio: 100%|██████████| 2/2 [00:00<00:00, 276.94it/s]\n"
     ]
    }
   ],
   "source": [
    "timit_test_clean = TimitDataset(TIMIT_DIR_8kHZ, subset='test', sample_rate=SAMPLE_RATE, with_path=False)\n",
    "noises_test = CachedWavSet(TEST_NOISE_DIR, sample_rate=SAMPLE_RATE, precache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_test = FixedMixtureSet(timit_test_clean, noises_test, snrs=TEST_SNRS, random_seed=SEED, with_snr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(batch, metrics, including='output', sample_rate=8000):\n",
    "    mix, clean, estimate, snr = batch\n",
    "    metrics = get_metrics(mix, clean, estimate, sample_rate=sample_rate,\n",
    "                          metrics_list=metrics, including=including)\n",
    "    metrics['snr'] = snr[0]\n",
    "    return metrics\n",
    "        \n",
    "def model_eval_iterator(model, test_set, num_workers=4):\n",
    "    loader = DataLoader(test_set, num_workers=num_workers)\n",
    "    \n",
    "    if model is not None:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    def model_eval(mix):\n",
    "        if model is None:\n",
    "            return mix\n",
    "        else:\n",
    "            return model(mix.cuda()).detach().cpu()\n",
    "    \n",
    "    for mix, clean, snr in tqdm(loader, 'Loading/enhancing data'):\n",
    "        enh = model_eval(mix)\n",
    "        yield mix.numpy(), clean.numpy(), enh.numpy(), snr\n",
    "            \n",
    "\n",
    "def evaluate_model(model, test_set, num_workers=4, metrics=['pesq', 'stoi', 'si_sdr'],\n",
    "                   sample_rate=8000):\n",
    "    df = pd.DataFrame(columns=['snr']+metrics)\n",
    "    ds_len = len(test_set)\n",
    "    including = 'input' if model is None else 'output'\n",
    "    eval_iter = model_eval_iterator(model, test_set, num_workers=num_workers)\n",
    "    eval_func = partial(_eval, metrics=metrics, including=including, sample_rate=sample_rate)\n",
    "    \n",
    "    with ProcessPoolExecutor(num_workers) as pool:        \n",
    "        for res in tqdm(pool.map(eval_func, eval_iter), 'Evaluating and calculating scores', total=ds_len):\n",
    "            df.append(res)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_input(*args, **kwargs):\n",
    "    return evaluate_model(None, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(batch, metrics, including='output', sample_rate=8000):\n",
    "    mix, clean, estimate, snr = batch\n",
    "    metrics = get_metrics(mix, clean, estimate, sample_rate=sample_rate,\n",
    "                          metrics_list=metrics, including=including)\n",
    "    metrics['snr'] = snr[0]\n",
    "    return metrics\n",
    "\n",
    "def data_feed_process(queue, model, test_set):\n",
    "    loader = DataLoader(test_set)\n",
    "    \n",
    "    if model is not None:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    def model_eval(mix):\n",
    "        if model is None:\n",
    "            return mix\n",
    "        else:\n",
    "            return model(mix.cuda()).detach().cpu()\n",
    "    \n",
    "    for mix, clean, snr in loader:\n",
    "        enh = model_eval(mix)\n",
    "        queue.put((mix.numpy(), clean.numpy(), enh.numpy(), snr))\n",
    "        \n",
    "def eval_process(input_queue, output_queue, **kwargs):\n",
    "    while True:\n",
    "        try:\n",
    "            batch = input_queue.get()\n",
    "            res = _eval(batch, **kwargs)\n",
    "            output_queue.put(res)\n",
    "        except Empty:\n",
    "            break\n",
    "\n",
    "def evaluate_model(model, test_set, num_workers=None, metrics=['pesq', 'stoi', 'si_sdr'],\n",
    "                   sample_rate=8000, max_queue_size=1000):\n",
    "    \n",
    "    if num_workers is None:\n",
    "        num_workers = cpu_count()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['snr']+metrics)\n",
    "    ds_len = len(test_set)\n",
    "    including = 'input' if model is None else 'output'\n",
    "        \n",
    "    input_queue = Queue(maxsize=max_queue_size)\n",
    "    output_queue = Queue(maxsize=max_queue_size)\n",
    "    \n",
    "    feed_pr = Process(target=data_feed_process, args=(input_queue, model, test_set))\n",
    "    feed_pr.start()\n",
    "    \n",
    "    eval_prs = []\n",
    "    for i in range(num_workers-1):\n",
    "        pr = Process(target=eval_process, args=(input_queue, output_queue), kwargs={\n",
    "            'metrics': metrics,\n",
    "            'including': including,\n",
    "            'sample_rate': sample_rate\n",
    "        })\n",
    "        pr.start()\n",
    "        eval_prs.append(pr)\n",
    "    \n",
    "    for i in tqdm(range(ds_len), 'Evaluating and calculating scores'):\n",
    "        res = output_queue.get()\n",
    "        df.append(res, ignore_index=True)\n",
    "        \n",
    "    feed_pr.join()\n",
    "    for pr in eval_prs:\n",
    "        pr.join()\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate_input(*args, **kwargs):\n",
    "    return evaluate_model(None, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_scores = evaluate_input(timit_test, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(dfs, te_snr, plot_name=None, figsize=(6,4), ax=None, legend=True): \n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "    else:\n",
    "        plt.sca(ax)\n",
    "        \n",
    "    for model_name, df in model_names:\n",
    "        labels[model_name] = label\n",
    "        dfs[model_name] = calculate_pesq(workspace, speech_dir=speech_dir, te_snr=te_snr,\n",
    "                                         model_name=model_name, calc_mixed=(model_name=='input'),\n",
    "                                         library=library, **kwargs)\n",
    "    \n",
    "    pesqs = {}\n",
    "    for model_name, df in dfs.items():\n",
    "        pesqs[model_name] = df.groupby('snr').agg({'pesq': ['mean']})['pesq']['mean']\n",
    "        \n",
    "    for model_name, series in pesqs.items():\n",
    "        line_kwargs = {'marker': 'o', 'alpha': 0.8}\n",
    "        if model_name == 'input':\n",
    "            line_kwargs = {'c': 'black', 'ls': '--'}\n",
    "        plt.plot(series.index, series, label=labels[model_name], **line_kwargs)\n",
    "    \n",
    "    plt.grid(which='both')\n",
    "    #plt.ylabel('PESQ' if library.endswith('pesq') else library.upper())\n",
    "    plt.title('PESQ' if library.endswith('pesq') else library.upper())\n",
    "    plt.xlabel('SNR, dB')\n",
    "    #plt.title('Test noises n121-122')\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "    if plot_name is not None:\n",
    "        plt.savefig(plot_name, bbox_inches='tight')\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, test_set, num_workers=10, metrics=['pesq', 'stoi', 'si_sdr']):\n",
    "#     df = pd.DataFrame(columns=['snr'] + metrics)\n",
    "#     if model is None:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
